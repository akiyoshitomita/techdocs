---
sidebar_position: 101
---

# 大規模データ処理の基本

本章ではなぜ大規模データ処理が必要なのか、どの様な概念で動作しているかについて説明します。

## 大規模データ処理の必要性

コンピューターの性能は年々高くなってきてはいますが、コンピューターで処理を行うデータ量も増えてきています。
１台のコンピューターで処理できるデータ量には限界があり必要な計算を１台のコンピュータでは行うことができなくなってきています。  
そこで、同じ処理を複数台のコンピューターを利用して計算をすることにより、目標となる時間内に計算を完了させることが必要になりました。  
この目標時間内に複数のコンピュータリソースで計算を行う為のフレームワークとして有名なのがhadoopやkafkaと言ったプラットフォームです。
特徴はそれぞれ異なりますが、大きな概念は同じです。

## 大規模データ処理の概念

大規模のデータを分散処理を行う場合、「データの分別」を行ったあとに「データ処理」を行います。  
データの分別は、データ一つ一つにキーを追加します。同じキーを纏めたデータの集合体をデータ処理を行うサーバに送ります。  
同じキーを持つデータは同一のサーバに送られデータ処理を行います。この動作は非常に重要で、前後の関連性を計算する場合に役に立ちます。  
例えば、店舗ごとの売上の合計を計算する場合など、店舗IDをキーにすることで同じ店舗のデータが同じデータ処理サーバに送られるので 他のデータを確認する必要がなくなり結果を出すことができます。  
逆に言うと、データをまとめる単位ごとに同じキーにしないとうまく分散することができなくなるので注意です。

![bigdata](bigdata.png)

:::note
画像はデータを中心とした概念図です。
実際のデータ処理には複数のデータを組み合わせることが多くもっと複雑になっています。他のサイトではデータを分別するところから処理するところの矢印がメッシュ状に描かれていることが多いですが、それらの図は処理フロー中心にしたフロー図になり抽象化の仕方が異なります。
:::
