---
sidebar_position: 101
---

# 大規模データ処理の基本

本章ではなぜ大規模データ処理が必要なのか、どの様な概念で動作しているかについて説明します。

## 大規模データ処理の必要性

コンピューターの性能は年々高くなってきてはいますが、コンピューターで処理を行うデータ量も増えてきています。
１台のコンピューターで処理できるデータ量には限界があり必要な計算を１台のコンピュータでは行うことができなくなってきています。  
そこで、同じ処理を複数台のコンピューターを利用して計算をすることにより、目標となる時間内に計算を完了させることが必要になりました。  
この目標時間内に複数のコンピュータリソースで計算を行う為のフレームワークとして有名なのがhadoopやkafkaと言ったプラットフォームです。
特徴はそれぞれ異なりますが、大きな概念は同じです。

## 大規模データ処理の概念

大規模のデータを分散処理を行う場合、「データの分別」を行ったあとに「データ処理」を行います。  
データの分別は、データ一つ一つにキーを追加します。同じキーを纏めたデータの集合体をデータ処理を行うサーバに送ります。  
同じキーを持つデータは同一のサーバに送られデータ処理を行います。この動作は非常に重要で、前後の関連性を計算する場合に役に立ちます。  
例えば、店舗ごとの売上の合計を計算する場合など、店舗IDをキーにすることで同じ店舗のデータが同じデータ処理サーバに送られるので 他のデータを確認する必要がなくなり結果を出すことができます。  
逆に言うと、データをまとめる単位ごとに同じキーにしないとうまく分散することができなくなるので注意です。

